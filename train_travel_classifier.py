"""
Script d'entra√Ænement pour le classificateur d'images de voyage
Utilise Transfer Learning avec ResNet18 pr√©-entra√Æn√© sur ImageNet
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, models, transforms
import time
import copy
from pathlib import Path
import json

# Configuration
DATASET_DIR = "ai_datasets/travel_images"
MODEL_SAVE_PATH = "models/travel_classifier.pth"
BATCH_SIZE = 32
NUM_EPOCHS = 10
LEARNING_RATE = 0.001

# Cat√©gories (doit correspondre aux dossiers)
CLASSES = ['beach', 'city', 'monument', 'mountain', 'nature', 'restaurant']

def get_data_transforms():
    """
    Transformations pour augmentation de donn√©es
    """
    data_transforms = {
        'train': transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(10),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'validation': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'test': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    }
    return data_transforms


def load_datasets():
    """
    Charge les datasets train/validation/test
    """
    print("üìÇ Chargement des datasets...")
    
    data_transforms = get_data_transforms()
    
    image_datasets = {
        x: datasets.ImageFolder(
            Path(DATASET_DIR) / x,
            data_transforms[x]
        )
        for x in ['train', 'validation', 'test']
    }
    
    dataloaders = {
        x: DataLoader(
            image_datasets[x],
            batch_size=BATCH_SIZE,
            shuffle=(x == 'train'),
            num_workers=0  # 0 pour Windows, augmentez sur Linux
        )
        for x in ['train', 'validation', 'test']
    }
    
    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'validation', 'test']}
    
    print("\nüìä STATISTIQUES DU DATASET")
    print("‚îÅ" * 60)
    for split, size in dataset_sizes.items():
        print(f"   {split.capitalize():12} : {size:4} images")
    print(f"   {'TOTAL':12} : {sum(dataset_sizes.values()):4} images")
    print("‚îÅ" * 60 + "\n")
    
    return dataloaders, dataset_sizes, image_datasets['train'].classes


def create_model(num_classes):
    """
    Cr√©e le mod√®le avec Transfer Learning (ResNet18)
    """
    print("üß† Cr√©ation du mod√®le (Transfer Learning avec ResNet18)...")
    
    # Charger ResNet18 pr√©-entra√Æn√© sur ImageNet
    model = models.resnet18(pretrained=True)
    
    # Geler les couches pr√©-entra√Æn√©es (optionnel)
    # for param in model.parameters():
    #     param.requires_grad = False
    
    # Remplacer la derni√®re couche pour nos 6 classes
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, num_classes)
    
    print(f"   ‚úÖ Mod√®le cr√©√© avec {num_classes} classes de sortie")
    print(f"   ‚úÖ Bas√© sur ResNet18 (pr√©-entra√Æn√© sur ImageNet)\n")
    
    return model


def train_model(model, dataloaders, dataset_sizes, num_epochs=10):
    """
    Entra√Æne le mod√®le
    """
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"üíª Device utilis√© : {device}")
    if device.type == 'cpu':
        print("   ‚ö†Ô∏è  Pas de GPU d√©tect√©, entra√Ænement sur CPU (plus lent)")
    print()
    
    model = model.to(device)
    
    # Loss et optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    
    print("üöÄ D√âBUT DE L'ENTRA√éNEMENT")
    print("=" * 60 + "\n")
    
    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")
        print("-" * 60)
        
        # Chaque epoch a une phase d'entra√Ænement et de validation
        for phase in ['train', 'validation']:
            if phase == 'train':
                model.train()
            else:
                model.eval()
            
            running_loss = 0.0
            running_corrects = 0
            
            # It√©rer sur les donn√©es
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)
                
                optimizer.zero_grad()
                
                # Forward
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)
                    
                    # Backward + optimize seulement en train
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
            
            if phase == 'train':
                scheduler.step()
            
            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]
            
            print(f"   {phase.capitalize():12} Loss: {epoch_loss:.4f}  Acc: {epoch_acc:.4f}")
            
            # Sauvegarder le meilleur mod√®le
            if phase == 'validation' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
                print(f"   ‚úÖ Nouveau meilleur mod√®le ! Accuracy: {best_acc:.4f}")
        
        print()
    
    print("=" * 60)
    print(f"‚úÖ ENTRA√éNEMENT TERMIN√â !")
    print(f"   Meilleure validation accuracy : {best_acc:.4f}")
    print("=" * 60 + "\n")
    
    # Charger le meilleur mod√®le
    model.load_state_dict(best_model_wts)
    return model, best_acc


def test_model(model, dataloaders):
    """
    Teste le mod√®le sur le test set
    """
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()
    
    running_corrects = 0
    total = 0
    
    print("üß™ TEST DU MOD√àLE")
    print("=" * 60)
    
    with torch.no_grad():
        for inputs, labels in dataloaders['test']:
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            
            running_corrects += torch.sum(preds == labels.data)
            total += labels.size(0)
    
    test_acc = running_corrects.double() / total
    print(f"   Test Accuracy : {test_acc:.4f} ({test_acc*100:.2f}%)")
    print("=" * 60 + "\n")
    
    return test_acc


def save_model(model, class_names, accuracy):
    """
    Sauvegarde le mod√®le entra√Æn√©
    """
    # Cr√©er le dossier models
    Path("models").mkdir(exist_ok=True)
    
    # Sauvegarder le mod√®le
    model_data = {
        'model_state_dict': model.state_dict(),
        'class_names': class_names,
        'num_classes': len(class_names),
        'accuracy': float(accuracy),
        'model_architecture': 'ResNet18',
        'input_size': 224,
        'trained_date': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    torch.save(model_data, MODEL_SAVE_PATH)
    
    print("üíæ SAUVEGARDE DU MOD√àLE")
    print("=" * 60)
    print(f"   Fichier : {MODEL_SAVE_PATH}")
    print(f"   Accuracy : {accuracy:.4f}")
    print(f"   Classes : {', '.join(class_names)}")
    print("=" * 60 + "\n")
    
    # Sauvegarder aussi les m√©tadonn√©es en JSON
    metadata = {
        'class_names': class_names,
        'num_classes': len(class_names),
        'accuracy': float(accuracy),
        'model_architecture': 'ResNet18',
        'trained_date': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open('models/travel_classifier_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print("‚úÖ Mod√®le sauvegard√© avec succ√®s !\n")


if __name__ == "__main__":
    print("\n" + "="*60)
    print("ü§ñ ENTRA√éNEMENT DU CLASSIFICATEUR D'IMAGES DE VOYAGE")
    print("="*60 + "\n")
    
    # V√©rifier que le dataset existe
    if not Path(DATASET_DIR).exists():
        print("‚ùå ERREUR : Le dataset n'existe pas !")
        print("   Ex√©cutez d'abord : python create_travel_dataset.py")
        exit(1)
    
    # Charger les donn√©es
    dataloaders, dataset_sizes, class_names = load_datasets()
    
    # Cr√©er le mod√®le
    model = create_model(len(class_names))
    
    # Entra√Æner
    model, best_acc = train_model(model, dataloaders, dataset_sizes, NUM_EPOCHS)
    
    # Tester
    test_acc = test_model(model, dataloaders)
    
    # Sauvegarder
    save_model(model, class_names, test_acc)
    
    print("üéâ PROCESSUS TERMIN√â !")
    print("   Vous pouvez maintenant int√©grer le mod√®le dans Django.\n")


